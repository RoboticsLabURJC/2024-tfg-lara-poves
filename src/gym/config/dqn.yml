CartPole-v1:
  n_timesteps: 500000
  policy: 'MlpPolicy'
  learning_rate: 2.3e-3
  batch_size: 64 # Experiences per batch (N)
  buffer_size: 100000 # Experiences in replay memory
  learning_starts: 1000
  gamma: 0.99
  target_update_interval: 10 # (F)
  train_freq: 256
  gradient_steps: 128 
  exploration_fraction: 0.16 # Descenso gradual durante el 16% del tiempo
  exploration_final_eps: 0.04 # Final value of epsilon
  tau: 1 # Soft update coefficient (Polyak update), if 1 hard update
  policy_kwargs: 
    net_arch: [256, 256] # 2 hidden layers of 256 neurons

MountainCar-v0:
  n_timesteps: 550000
  policy: 'MlpPolicy'
  learning_rate: 0.004
  batch_size: 128
  buffer_size: 10000
  learning_starts: 1000
  gamma: 0.98
  target_update_interval: 600
  train_freq: 16 # Actualiza el modelo cada 16 pasos
  gradient_steps: 8 # Numero de actualizaciones que se hacen en cada rollout
  exploration_fraction: 0.2
  exploration_final_eps: 0.07
  policy_kwargs: 
    net_arch: [256, 256]

Acrobot-v1:
  n_timesteps: 600000
  policy: 'MlpPolicy'
  learning_rate: 6.3e-4
  batch_size: 128
  buffer_size: 50000
  learning_starts: 0
  gamma: 0.99
  target_update_interval: 250
  train_freq: 4
  gradient_steps: -1 # Igual que train_freq
  exploration_fraction: 0.12
  exploration_final_eps: 0.1
  policy_kwargs: 
    net_arch: [256, 256]