PPO:
  policy: "MultiInputPolicy"
  learning_rate: 0.0005
  gamma: 0.8
  gae_lambda: 0.9 # γ
  n_steps: 64 # The number of steps to run for each environment per update
  batch_size: 64 # = n_steps * n_envs(=1)
  ent_coef: 0.01 # β entropy -> exploration vs explotation
  clip_range: 0.2 # epsilon [0.1, 0.3]
  n_timesteps: 5_000_000
  n_epochs: 3 # number of times a experience is use [3, 10]