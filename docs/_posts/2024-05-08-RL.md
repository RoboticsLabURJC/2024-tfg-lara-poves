---
title: "Aprendizaje por refuerzo"
last_modified_at: 2024-05-10T11:41:00
categories:
  - Blog
tags:
  - Deep Reinforcement Learning
  - Q-learning
  - Aprendizaje automático
---

## Índice
1. [Tipos de entrenamientos](#tipos-de-entrenamiento)
  - [Entrenamiento supervisado](#entrenamiento-supervisado)
  - [Entrenamiento no supervisado](#entrenamiento-no-supervisado)
  - [Entrenamiento semi-supervisado](#entrenamiento-semi-supervisado)
2. [Q-learning](#q-learning)
  - [Conceptos básicos RL](#conceptos-básicos-rl)
  - [Algoritmo de Q-learning](#algoritmo-de-q-learning)
  - [Q-Networks](#q-networks)
3. [Deep Reinforcement Learning](#deep-reinforcement-learning)
  - [DQN](#dqn)
    - [Experience relay](#experience-relay)

## Tipos de entrenamiento 

En el contexto de aprendizaje automático, existen varios tipos de entrenamiento: supervisado, no supervisado y semi-supervisados.
<figure class="align-center" style="max-width: 100%">
  <img src="{{ site.url }}{{ site.baseurl }}/images/RL/training_type.png" alt="">
</figure>

### Entrenamiento supervisado
El conjunto de datos de entrenamiento es etiquetado, cada una de las entradas tiene una salida asociada a la respuesta correcta según el modelo que quiere predecir. Durante el entrenamiento, el modelo ajusta sus parámetros para minimizar el error entre las predicciones y las etiquetas reales. El objetivo de este algoritmo de aprendizaje es predecir etiquetas para datos no visto anteriormente, es ampliamente utilizado en tareas de clasificación y regresión. La precisión del modelo depende de la calidad de los datos de entrenamiento.

### Entrenamiento no supervisado
El aprendizaje no supervisado opera en conjuntos de datos sin etiquetas, donde el objetivo del modelo es identificar patrones y estructuras, agrupando los datos en categorías o clústeres. Las tareas principales incluyen el *clustering* y la reducción de dimensionalidad. Este enfoque puede ser más rentable que el aprendizaje supervisado, ya que no requiere la creación y etiquetado de grandes conjuntos de datos de entrenamiento. En este tipo de aprendizaje, la calidad de los resultados depende en gran medida de la elección adecuada del algoritmo y los parámetros utilizados.

### Entrenamiento semi-supervisado
El aprendizaje semi-supervisado emplea una combinación de datos etiquetados y no etiquetados en el conjunto de entrenamiento. Este enfoque es útil cuando el etiquetado de datos es costoso o difícil de obtener en grandes cantidades. A diferencia del aprendizaje supervisado tradicional, aquí se pueden lograr resultados significativos con solo unos pocos ejemplos etiquetados, lo que hace que el proceso sea más eficiente y práctico en ciertos escenarios. Este proceso se asemeja más a cómo aprendemos las personas.

## Q-learning

*Q-learning* es una rama del aprendizaje por refuerzo o **RL** (*reinforcement learnin*), el cual se enfoca en aprender a tomar decisiones secuenciales para maximizar una recompensa acumulativa mediante la experiencias y observaciones del entorno. 

### Conceptos básicos RL
- **Agente**: entidad que toma las decisiones.
- **Entorno**: mundo en el que opera el agente.
- **Estado**: donde está el agente en el entorno. Al definir los posibles estados del agente, debemos considerar qué información necesita para tomar decisiones, representada por un vector de variables relevantes. Aunque debemos tener en cuenta que algunos de estos estados pueden ser inalcanzables, por ejemplo, cuando el pasajero está en su destino pero el taxi se encuentra en cualquier otra ubicación, pero se representan todos por simplicidad.
- **Acción**: el siguiente movimiento que va a hacer el agente. El estado en el que está el agente determina las posibles acciones a tomar, por ejemplo, un coche no puede girar a la izquierda si en ese lado hay un muro.
- **Recompensa**: *feedback* que el agente recibe sobre el entorno al tomar una acción. 
- **Política**: es una función que mapea los estados del agente a las acciones que debe tomar. En *Q-learning*, se refiere a la consulta de una tabla denominada *Q-table*, este algoritmo se basa en valores (*value-based*).
- **Valor**: representa cuán bueno es tomar una acción específica en un determinado estado, este valor se calcula teniendo en cuenta las posibles recompensas futuras que el agente podría recibir y la probabilidad de alcanzar esos estados futuros.

En un espacio de estados discreto, el éxito se alcanza al llegar al estado objetivo. Sin embargo, en un espacio de estados continuo, el objetivo se logra al mantener un conjunto específico de estados durante un tiempo prolongado.

### Algoritmo de Q-learning
El proceso de toma de decisiones implica determinar el estado actual, seleccionar una acción según la política y recibir su recompensa. Matemáticamente, esto se expresa a través del proceso de decisión de Markov (MDP), que se fundamenta en las cadenas de Markov: un modelo de sistema donde la probabilidad del siguiente estado depende únicamente del estado actual.
<figure class="align-center" style="max-width: 100%">
  <img src="{{ site.url }}{{ site.baseurl }}/images/RL/Qlearning/process.png" alt="">
</figure>

Debemos tener en cuenta que estamos en un entorno **estocástico**, las acciones del agente deben tener cierto grado de aleatoriedad. Inicialmente, cuando el agente carece de información, la *Q-table* está vacía y todas las acciones tienen la misma probabilidad. Sin embargo, a medida que interactúa con el entorno, la tabla se completa y las acciones con recompensas más altas se vuelven más probables. El proceso culmina cuando la tabla converge, es decir, los valores dejan de actualizarse, momento en el que hemos encontrado la solución más óptima al problema.
<figure class="align-center" style="max-width: 100%">
  <img src="{{ site.url }}{{ site.baseurl }}/images/RL/Qlearning/QTable.png" alt="">
</figure>

La función que determina las decisiones que toma el agente es la ecuación de **Bellman**: 
<p style="text-align:center; font-weight:bold;">
  newQ(s, a) = Q(s, a) + α[R(s, a) + γmaxQ'(s', a') - Q(s, a)]
</p>
Donde Q(s,a) es el valor del estado actual, R(s, a) esta la recompensa recibida y maxQ'(s', a') es el valor máximo entre todas las posibles decisiones del agente desde el nuevo estado. 

Los hiperparámetros α, γ y ε están en el rango de 0 a 1:
- **Alpha**: el ratio de aprendizaje. Un valor de 0 indica que el agente no adquiere conocimiento, mientras que 1 implica entornos deterministas. La reducción gradual de alpha a lo largo del tiempo previene el *overfitting*.
- **Epsilon**: el ratio de exploración. Al principio, el agente debe explorar diferentes acciones para encontrar las que producen mayor recompensa, pero con el tiempo, debe priorizar la explotación de las mejores acciones. Reducir ε con el tiempo nos permite conseguir este enfoque y que la *Q-table* converja de manera óptima.
- **Gamma**: el ratio de descuento, que determina cómo el agente valora las recompensas futuras en relación con las inmediatas. Un valor de gamma cercano a 1 significa que el agente valora mucho las recompensas futuras, mientras que un valor cercano a 0 indica que se enfoca principalmente en las recompensas inmediatas. A medida que nos acercamos al objetivo, es preferible aprovechar las recompensas a corto plazo en lugar de esperar por las futuras, que pueden no estar disponibles una vez que se complete la tarea.

### Q-Networks
Una *Q-table* ya no es una forma práctica de modelar la función de transición de estado-acción, especialmente cuando el espacio de estados es muy grande. En su lugar, utilizaremos una *Q-network*, que es un tipo de red neuronal diseñada para aproximar los *Q-values*. Con una *Q-table*, almacenamos los valores en una tabla, mientras que con una *Q-network*, almacenamos esta información en los pesos, que actúan como coeficientes en la que mapea estados a acciones (*Q-function*). Este último tipo de agente se clasifica como *policy-agent*, la principal diferencia con un *value-agent*, es que son capaces de generalizar sobre el entorno, pueden hacer predicciones sobre estados que aún no han visto.

Una Q-network se caracteriza por su estructura neuronal simple, compuesta por una única capa de y una función de pérdida. Recibe como entrada los estados del entorno y genera como salida los *Q-values* asociados a cada acción posible. La función de pérdida evalúa la diferencia entre los *Q-values* predichos y los *Q-values* reales, la cual se utiliza para actualizar los pesos de la red mediante retropropagación.

## Deep Reinforcement Learning

### DQN
Una implmentación de un DQN (*Deep Q-Network*) utilizando la biblioteca TensorFlow podría ser:
```python
self.model = Sequential()
self.model.add.(Dense(24, input_shape=(observation_space,), activation="relu"))
self.model.add(Dense(24, activation="relu"))
self.model.add(Dense(self.action_space, activation="linear"))
self.model.compile(loss="mse", optimizer=Adam(lr=learning_rate_adam))
```

#### Experience relay
Esta técnica se basa en la no actualización de los *Q-values* en cada paso, lo que contribuye a reducir el ruido en el modelo y facilita su capacidad de generalización. Ahora, las actualizaciones se realizan en lotes seleccionados al azar, lo que garantiza la inclusión de experiencias provenientes de distintas etapas del historial de interacción del agente en cada grupo. Este enfoque ayuda a disminuir las correlaciones entre actualizaciones sucesivas y proporciona un conjunto de datos de entrenamiento más variado.
