---
title: "Aprendizaje por refuerzo"
last_modified_at: 2024-05-13T20:32:00
categories:
  - Blog
tags:
  - Deep Reinforcement Learning
  - Q-learning
  - Aprendizaje automático
---

## Índice
1. [Tipos de entrenamientos](#tipos-de-entrenamiento)
  - [Entrenamiento supervisado](#entrenamiento-supervisado)
  - [Entrenamiento no supervisado](#entrenamiento-no-supervisado)
  - [Entrenamiento semi-supervisado](#entrenamiento-semi-supervisado)
2. [Q-learning](#q-learning)
  - [Conceptos básicos RL](#conceptos-básicos-rl)
  - [Algoritmo de Q-learning](#algoritmo-de-q-learning)
    - [MDP](#mdp)
    - [TD](#td)
3. [Deep Reinforcement Learning](#deep-reinforcement-learning)
  - [DQN](#dqn)
    - [Experience relay](#experience-relay)

## Tipos de entrenamiento 

En el contexto de aprendizaje automático, existen varios tipos de entrenamiento: supervisado, no supervisado y semi-supervisados.
<figure class="align-center" style="max-width: 100%">
  <img src="{{ site.url }}{{ site.baseurl }}/images/RL/training_type.png" alt="">
</figure>

### Entrenamiento supervisado
El conjunto de datos de entrenamiento es etiquetado, cada una de las entradas tiene una salida asociada a la respuesta correcta según el modelo que quiere predecir. Durante el entrenamiento, el modelo ajusta sus parámetros para minimizar el error entre las predicciones y las etiquetas reales. El objetivo de este algoritmo de aprendizaje es predecir etiquetas para datos no visto anteriormente, es ampliamente utilizado en tareas de clasificación y regresión. La precisión del modelo depende de la calidad de los datos de entrenamiento.

### Entrenamiento no supervisado
El aprendizaje no supervisado opera en conjuntos de datos sin etiquetas, donde el objetivo del modelo es identificar patrones y estructuras, agrupando los datos en categorías o clústeres. Las tareas principales incluyen el *clustering* y la reducción de dimensionalidad. Este enfoque puede ser más rentable que el aprendizaje supervisado, ya que no requiere la creación y etiquetado de grandes conjuntos de datos de entrenamiento. En este tipo de aprendizaje, la calidad de los resultados depende en gran medida de la elección adecuada del algoritmo y los parámetros utilizados.

### Entrenamiento semi-supervisado
El aprendizaje semi-supervisado emplea una combinación de datos etiquetados y no etiquetados en el conjunto de entrenamiento. Este enfoque es útil cuando el etiquetado de datos es costoso o difícil de obtener en grandes cantidades. A diferencia del aprendizaje supervisado tradicional, aquí se pueden lograr resultados significativos con solo unos pocos ejemplos etiquetados, lo que hace que el proceso sea más eficiente y práctico en ciertos escenarios. Este proceso se asemeja más a cómo aprendemos las personas.

## Q-learning

*Q-learning* es una rama del aprendizaje por refuerzo o **RL** (*reinforcement learnin*), el cual se enfoca en aprender a tomar decisiones secuenciales para maximizar una recompensa acumulativa mediante la experiencias y observaciones del entorno.
<figure class="align-center" style="max-width: 100%">
  <img src="{{ site.url }}{{ site.baseurl }}/images/RL/Qlearning/process.png" alt="">
</figure>

### Conceptos básicos RL
- **Agente**: entidad que toma las decisiones.
- **Entorno**: mundo en el que opera el agente.
- **Estado**: donde está el agente en el entorno. Al definir los posibles estados del agente, debemos considerar qué información necesita para tomar decisiones, representada por un vector de variables relevantes. Aunque debemos tener en cuenta que algunos de estos estados pueden ser inalcanzables, por ejemplo, cuando el pasajero está en su destino pero el taxi se encuentra en cualquier otra ubicación, pero se representan todos por simplicidad.
- **Acción**: el siguiente movimiento que va a hacer el agente. El estado en el que está el agente determina las posibles acciones a tomar, por ejemplo, un coche no puede girar a la izquierda si en ese lado hay un muro.
- **Recompensa**: *feedback* que el agente recibe sobre el entorno al tomar una acción. 
- **Valor**: representa cuán bueno es tomar una acción específica en un determinado estado, este valor se calcula teniendo en cuenta las posibles recompensas futuras que el agente podría recibir y la probabilidad de alcanzar esos estados futuros.
- **Política**: es una función que mapea los estados del agente a las acciones que debe tomar. Existen tres tipos principales: *value-based*, *policy-based* y *model-based*. Las políticas basadas en valor o en política se benefician del uso de **DP** (*Dynamic Programming*), ya que conocemos las dinámicas del entorno. Por otro lado, en las políticas basadas en modelo, desconocemos las dinámicas del entorno, por lo que usamos el método Monte Carlo. Sin embargo, este algoritmo se limita a tareas episódicas, ya que necesitamos un inicio y un final definidos. Esto permite que el agente realice múltiples episodios de forma aleatoria y recopile suficiente experiencia para estimar con precisión la función de valor o la política calculando el promedio de la información adquirida.

En un entorno de estados episódicos, el éxito se define por alcanzar el estado objetivo al final de un episodio. Sin embargo, en tareas continuas, no hay un límite claro de episodios. En su lugar, el éxito se determina al mantener un conjunto específico de estados durante un período prolongado. Por ejemplo, en el caso de un péndulo, el objetivo podría ser mantener la inclinación dentro de un rango determinado durante un tiempo prolongado.

### Algoritmo de Q-learning

#### MDP
Debemos tener en cuenta que estamos en un entorno **estocástico**, la probabilidad del siguiente estado depende únicamente  del estado actual. Matemáticamente, esto se expresa a través del proceso de decisión de Markov (**MDP**), que se fundamenta en las cadenas de Markov.
```python
for episode in range(MAX_EPISODE):
  state = env.reset()
  agent.reset()

  for t in range(MAX_STEP):
    action = agent.act(state)
    state, reward = env.step(action)
    agent.update(action, state, reward)

    if env.done():
      break
```
#### TD
El algortimo TD (*Temporal Difference*) coge los beneficios del método Monte Carlo y DP, no se requieren las dinámicas del entorno y nos sirve para espacios episódiscos y continuos. La política de un algoritmo de *Q-learning* se basa en los valores de una tabla que se van actualizando en cada iteración (*value-based*). Inicialmente, cuando el agente carece de información, la *Q-table* está vacía y todas las acciones tienen la misma probabilidad. Sin embargo, a medida que interactúa con el entorno, la tabla se completa y las acciones con recompensas más altas se vuelven más probables. El proceso culmina cuando la tabla converge, es decir, los valores dejan de actualizarse, momento en el que hemos encontrado la solución más óptima al problema. Por lo tanto, *Q-learning* es un algortimo ***value-based***.
<figure class="align-center" style="max-width: 75%">
  <img src="{{ site.url }}{{ site.baseurl }}/images/RL/Qlearning/QTable.png" alt="">
</figure>

La función que determina las decisiones que toma el agente es la **ecuación de Bellman**: 
<p style="text-align:center; font-weight:bold;">
  newQ(s, a) = Q(s, a) + α[R(s, a) + γmaxQ'(s', a') - Q(s, a)]
</p>
Donde Q(s, a) es el valor del estado actual, R(s, a) esta la recompensa recibida y maxQ'(s', a') es el valor máximo entre todas las posibles decisiones del agente desde el nuevo estado. El término que multiplica a *alpha* es el error de TD, el cual debemos intentar minimizar en cada iteración.

Los hiperparámetros *α*, *γ* y *ε* están en el rango de 0 a 1:
- **Alpha**: el ratio de aprendizaje. Un valor de 0 indica que el agente no adquiere conocimiento, mientras que 1 implica entornos deterministas. La reducción gradual de alpha a lo largo del tiempo previene el *overfitting*.
- **Gamma**: el ratio de descuento, que determina cómo el agente valora las recompensas futuras en relación con las inmediatas. Un valor de gamma cercano a 1 significa que el agente valora mucho las recompensas futuras, mientras que un valor cercano a 0 indica que se enfoca principalmente en las recompensas inmediatas. A medida que nos acercamos al objetivo, es preferible aprovechar las recompensas a corto plazo en lugar de esperar por las futuras, que pueden no estar disponibles una vez que se complete la tarea.
- **Epsilon**: el ratio de exploración. Al principio, el agente debe explorar diferentes acciones para encontrar las que producen mayor recompensa, pero con el tiempo, debe priorizar la explotación de las mejores acciones. Reducir ε con el tiempo nos permite conseguir este enfoque y que la *Q-table* converja de manera óptima. Esta estrategia se conoce como política *ε-greedy*:

```python
epsilon = 0.3
epsilon_decay = 0.99  # decreasing by 1% each time

if epsilon < threshold:
  explore()
else:
  exploit()

epsilon = epsilon * epsilon_decay
```

En este diagrama, se presenta la definición completa del algoritmo:
<figure class="align-center" style="max-width: 90%">
  <img src="{{ site.url }}{{ site.baseurl }}/images/RL/Qlearning/alg.png" alt="">
</figure>

## Deep Reinforcement Learning

### DQN
---
Una *Q-tables* ya no es una forma práctica de modelar la función de transición de estado-acción, especialmente cuando el espacio de estados es muy extenso, debido a que requeriríamos explorar cada estado al menos una vez para encontrar la mejor solución. En su lugar, utilizaremos una *Q-network*, que es un tipo de red neuronal diseñada para aproximar los *Q-values*. Esta red es capaz de estimar el *Q-value* de estados no explorados, ya que aprende las relaciones entre los diferentes pares estado-acción.

En una *Q-table*, almacenamos los valores en una tabla, mientras que en una *Q-network*, esta información se guarda en los pesos de la red, los cuales actúan como coeficientes en la función que mapea estados-acciones (*Q-function*). Una *Q-network* recibe los estados del entorno como entrada y produce como salida el *Q-value* de cada acción posible. También incluye una función de pérdida que evalúa la diferencia entre los *Q-values* predichos y reales, y se utiliza para actualizar los pesos de la red mediante retropropagación. Una implementación de un DQN utilizando la biblioteca *TensorFlow* podría ser la siguiente:
```python
self.model = Sequential()
self.model.add.(Dense(24, input_shape=(observation_space,), activation="relu"))
self.model.add(Dense(24, activation="relu"))
self.model.add(Dense(self.action_space, activation="linear"))
self.model.compile(loss="mse", optimizer=Adam(lr=learning_rate_adam))
```

#### Política Boltzmann
Como ya vimos con anterioridad en el aparatdo de *ε-greedy*, necesitamos un balance entre explotación y explotación. Esta técnica explora de manera aleatoria, con Boltzman pretendemos explorar con un mayor grado de conocimeinto, las acciones de q-values mas altos tienenn mas probabilidad de ser elegidas. Para ello utilizamos la función softmax parametrizada por **τ** , valores altos de τ definen una distribución más uniforme, las acciones tienen probabilidades más parecidas, se elige de manera mas aleatoria; mientras que valotes de τ bajos, hacen que la seleccion se concrente en los q values altos.
<p style="text-align:center; font-weight:bold;">
  pboltzmann(a | s) = exp(Q(s, a) / τ) / Σ(exp(Q(s, a') / τ))
</p>

Como ya discutimos en el apartado de *ε-greedy*, necesitamos encontrar un equilibrio entre la exploración y la explotación. Mientras que la técnica ε*-greedy* es totalmente aleatoria en la exploración, con la política de Boltzmann buscamos explorar con un mayor grado de conocimiento. En esta estrategia, las acciones con *Q-vañues* más altos tienen una probabilidad más alta de ser seleccionadas, para lograrlo, empleamos la función softmax parametrizada por **τ**. Cuando τ es alto, se obtiene una distribución de probabilidad más uniforme, lo que significa que todas las acciones tienen probabilidades similares de ser seleccionadas, lo que resulta en una exploración más aleatoria. Por otro lado, cuando τ es bajo, la selección se concentra en las acciones con *Q-values* más altos, lo que lleva a una mayor explotación de estas acciones.
<p style="text-align:left; font-weight:bold;">
  τ = 1   ->  p(x) = [0.27, 0.73] (softmax)
</p>
<p style="text-align:left; font-weight:bold;">
  τ = 5   ->  p(x) = [0.45, 0.55]
</p>
<p style="text-align:left; font-weight:bold;">
  τ = 0.5 ->  p(x) = [0.12, 0.88]
</p>

La política de Boltzmann también ofrece una relación más suave entre las estimaciones de los *Q-values* y las probabilidades de acción en comparación con una política *ε-greedy*. Por ejemplo, si consideramos dos posibles acciones con *Q-values* de Q(s, a1) = 5.05 y Q(s, a2) = 4.95, con la política *ε-greedy*, a2 tendría casi toda la probabilidad de ser elegida, mientras que con la política de Boltzmann, ambas acciones tendrían probabilidades muy similares.

Sin embargo, la política de Boltzmann tiene el riesgo de quedarse atrapada en un mínimo local. Por ejemplo, si tenemos Q(s, a1) = 2.5 y Q(s, a2) = -3, y a2 es la mejor opción, a pesar de ello, a2 tendría una probabilidad extremadamente baja de ser seleccionada con la política de Boltzmann, mientras que con la política *ε-greedy* esta probabilidad no sería tan pequeña. Este problema puede abordarse disminuyendo gradualmente el parámetro τ, pero debemos tener cuidado de no reducirlo demasiado rápido, ya que podríamos seguir quedando atrapados en mínimos locales.

#### Experience relay
Esta técnica consiste en crear una memoria de reproducción de experiencias que almacena las k experiencias más recientes que un agente ha recopilado, ya que son las más relevantes. Si la memoria está llena, se descarta la experiencia más antigua para dar espacio a la más reciente. En cada *step* de entrenamiento, se muestrea uno o más lotes de datos de forma aleatoria desde la memoria para actualizar los parámetros de la red. El valor de k suele ser bastante grande, entre 10,000 y 1,000,000, mientras que el número de elementos en un lote es mucho más pequeño, típicamente entre 32 y 2048.

El tamaño de la memoria debe ser lo suficientemente grande como para contener muchas experiencias de episodios. Cada lote típicamente contendrá experiencias de diferentes episodios y diferentes políticas, lo que descorrelaciona las experiencias utilizadas para entrenar a un agente. Esto, a su vez, reduce la varianza de las actualizaciones de parámetros, lo que ayuda a estabilizar el entrenamiento. No obstante, la memoria también debe ser lo suficientemente pequeña como para que cada experiencia tenga más probabilidades de ser muestreada más de una vez antes de ser descartada, lo que hace que el aprendizaje sea más eficiente.

#### Algoritmo DQN
```python
for m in range(MAX_STEPS): # Número de episodios
  a = get_action(s)
  r, s_next = exec_action(a)
  mem[h] = [s, a, r, s_next] # Guardamos la nueva experiencia

  for b in range(B): # Número total de lotes
    b = get_random_batch()

    for u in range(U): # Actualizaciones por lote
      sum = 0

      for i in range(N): # Número de experiencias por lote
        s, a, r, s_next = b[i]
        y = r + gamma * max(Q(s_next, a_next)) # Calcular Q-value
        sum += (y - Q(s, a)) ** 2

      L_w = 1 / N * sum # Función de pérdida
      weights = weights + alpha * gradient_w * L_w # Actualizar los pesos de la red
      
  tau = tau * decay_dactor # Decaer τ
```
Unos valores óptimos para los parámetros *U* y *B* dependen del problema y de los recursos computacionales disponibles, sin embargo, son comunes valores en el rango de 1 a 5.

Debemos resaltar que, para calcular el siguiente estado, es necesario seleccionar el valor máximo entre todas las posibles acciones del estado siguiente, por lo tanto, el **espacio de acciones debe ser discreto**.